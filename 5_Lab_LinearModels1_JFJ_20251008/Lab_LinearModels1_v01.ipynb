{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## EE512 – Applied Biomedical Signal Processing\n",
    "# Practical session – Linear Models I\n",
    "### Instructions\n",
    "* This notebook provides all the questions of the practical session and the space to answer them. We recommend working directly here and then exporting the document as your report.\n",
    "* Include any code used when addressing the questions, any figures you find useful, together with your answers.\n",
    "* Please submit your report as a single PDF file, named according to: `name1_name2_name3_LabLinearModelsI.pdf`\n",
    "* We recommend working in a group of 4–5 students (max 6); the report does not have to be submitted by every group member -- one submission per group is sufficient."
   ],
   "id": "c327e0398b754458"
  },
  {
   "cell_type": "code",
   "id": "730bc8ec-59db-49bd-9fca-b58974d7e1da",
   "metadata": {},
   "source": [
    "%matplotlib widget\n",
    "\n",
    "# Imports\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from statsmodels.tsa.ar_model import AutoReg, ar_select_order\n",
    "from statsmodels.regression.linear_model import yule_walker\n",
    "\n",
    "from scipy.signal import lfilter\n",
    "\n",
    "# File paths\n",
    "fbci = os.path.join(os.getcwd(), 'data', 'bci.json')\n",
    "fafs = os.path.join(os.getcwd(), 'data', 'AF_sync.dat')\n",
    "fspc = os.path.join(os.getcwd(), 'data', 'speech.dat')\n",
    "fbld = os.path.join(os.getcwd(), 'data', 'blood.dat')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "6f333a11-f901-4420-bb5d-1dce969a385d",
   "metadata": {},
   "source": [
    "### Experiment 1: classifying EEG signals\n",
    "The file `/data/bci.mat` contains two data matrices, `left_hand` and `right_foot`, from a brain-computer interface (BCI) experiment. Each column in these matrices corresponds to a 2-second electroencephalography (EEG) recording (sampling frequency of 128 Hz) from the same electrode. The recordings in `left_hand` (respectively `right_foot`) were performed while the subject imagines a movement of the left hand (resp. right foot). The goal of the BCI experiment is to be able to “guess” what is being imagined based on the EEG signals alone.\n",
    "\n",
    "We start by importing the signals (and removing their averages to better approximate our AR models):"
   ]
  },
  {
   "cell_type": "code",
   "id": "c72f635a-cdd4-4a98-9f85-9a689883e646",
   "metadata": {},
   "source": [
    "with open(fbci, 'r') as f:\n",
    "    jc = json.load(f)\n",
    "\n",
    "eeg_lh = np.array(jc['left_hand'])\n",
    "eeg_rf = np.array(jc['right_foot'])\n",
    "\n",
    "eeg_lh -= np.mean(eeg_lh, axis=0, keepdims=True)\n",
    "eeg_rf -= np.mean(eeg_rf, axis=0, keepdims=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "eadfb94d-b127-4965-9395-53abbcfadd64",
   "metadata": {},
   "source": [
    "#### AR model order\n",
    "A useful first step to look for structure in the signals is estimating the AR model order of each signal. We define a function `ar_order` to fit models of different orders, obtain the model noise variance, and apply a specific criterion:"
   ]
  },
  {
   "cell_type": "code",
   "id": "a94b34e3-5c2b-42d0-893f-66e3991f117f",
   "metadata": {},
   "source": [
    "def ar_order(x, omax, Aff=0):\n",
    "    \"\"\"\n",
    "    AR order estimation\n",
    "    x: signal\n",
    "    omax: maximum possible order\n",
    "    Aff: 0 no graphic display; 1 display\n",
    "    \n",
    "    Returns:\n",
    "    omdl: order estimated with MDL\n",
    "    \"\"\"\n",
    "    \n",
    "    nx = len(x)\n",
    "    s = np.zeros((omax,))\n",
    "    c = np.zeros((omax,))\n",
    "\n",
    "    for k in range(omax):\n",
    "        \n",
    "        n = k+1\n",
    "        \n",
    "        ar_model = AutoReg(x, n, trend='n')\n",
    "        ar_model_fit = ar_model.fit()\n",
    "        sg2 = ar_model_fit.sigma2\n",
    "        \n",
    "        s[k] = sg2\n",
    "        c[k] = nx * np.log(sg2) + (n+1) * np.log(nx)\n",
    "\n",
    "    if Aff == 1:\n",
    "        plt.figure()\n",
    "        plt.plot(range(1, omax+1), mdl, 'o-')\n",
    "        plt.title('Criterion')\n",
    "        plt.show()\n",
    "        \n",
    "    return np.argmin(c)+1, s, c"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "ca89e62f-955c-44b1-83b0-f15696fe5496",
   "metadata": {},
   "source": [
    "**Question 1.1.** What is the name of the criterion being applied in the function `ar_order` implemented above? How would you change the code to apply the Akaike Information Criterion (AIC) instead?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a582a5-4684-42f7-8764-a33d809bdaf2",
   "metadata": {},
   "source": [
    "**Answer 1.1.** `Write your answer here`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a810e9-9a41-4afe-b0f4-cb15b6e785a2",
   "metadata": {},
   "source": [
    "Let's look at the order estimate for an example signal, in a reasonable range up 20:"
   ]
  },
  {
   "cell_type": "code",
   "id": "1227a058-d68e-4a76-996d-13d0c322fd54",
   "metadata": {},
   "source": [
    "nmax = 20\n",
    "_, s, c = ar_order(eeg_lh[:,1], nmax)\n",
    "\n",
    "fig = plt.figure(constrained_layout=True)\n",
    "n = np.arange(1, nmax+1)\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(n, s)\n",
    "plt.xlabel('Model order')\n",
    "plt.ylabel('Noise variance')\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(n, c)\n",
    "plt.xlabel('Model order')\n",
    "plt.ylabel('Order estimation criterion')\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "715ef9fb-321c-4844-a94b-faf92f100bee",
   "metadata": {},
   "source": [
    "**Question 1.2.** Do the two curves obtained for this example signal behave as we would expect? What are their most important characteristics?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45236259-691f-4b1f-b90d-81d9c4828428",
   "metadata": {},
   "source": [
    "**Answer 1.2.** `Write your answer here`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f3d33c2-7de0-4343-acf0-c6e2515375c9",
   "metadata": {},
   "source": [
    "We can now estimate and print the optimal model order for every signal of each condition:"
   ]
  },
  {
   "cell_type": "code",
   "id": "e7a235e7-fb85-4146-b70a-ae90da961780",
   "metadata": {},
   "source": [
    "nmax = 10\n",
    "\n",
    "print(\"\\nLeft hand :\", end=\"\")\n",
    "for k in range(eeg_lh.shape[1]):\n",
    "    n, _, _ = ar_order(eeg_lh[:,k], nmax)\n",
    "    print(\"  {:02.0f}\".format(n), end=\"\")\n",
    "print(\"\")\n",
    "\n",
    "print(\"Right foot:\", end=\"\")\n",
    "for k in range(eeg_rf.shape[1]):\n",
    "    n, _, _ = ar_order(eeg_rf[:,k], nmax)\n",
    "    print(\"  {:02.0f}\".format(n), end=\"\")\n",
    "print(\"\\n\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "4939d90f-16d0-4da7-be6a-e5cb2dadf1b8",
   "metadata": {},
   "source": [
    "**Question 1.3.** Based on these AR order estimates, is there already a difference between the two categories overall? And if we wish to perform AR model estimation using a common choice of model order for all signals, which value should be chosen, and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92bcc149-833b-43de-b993-d91d7453e5ac",
   "metadata": {},
   "source": [
    "**Answer 1.3.** `Write your answer here`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec8fb0a-7efb-4fe6-bcf9-fe4bdd15bff5",
   "metadata": {},
   "source": [
    "#### Movement prediction\n",
    "We now look at the resulting AR coefficients when choosing a model order of `3`:"
   ]
  },
  {
   "cell_type": "code",
   "id": "62124a30-c913-4d15-997e-5badd02c5133",
   "metadata": {},
   "source": [
    "n = 3\n",
    "\n",
    "fig = plt.figure(constrained_layout=True)\n",
    "\n",
    "for k in range(eeg_lh.shape[1]):\n",
    "    ar_model = AutoReg(eeg_lh[:,k], n, trend='n')\n",
    "    ar_model_fit = ar_model.fit()\n",
    "    plt.plot(range(1,4), ar_model_fit.params, 'ro-', label='Left hand')\n",
    "    \n",
    "for k in range(eeg_rf.shape[1]):\n",
    "    ar_model = AutoReg(eeg_rf[:,k], n, trend='n')\n",
    "    ar_model_fit = ar_model.fit()\n",
    "    plt.plot(range(1,4), ar_model_fit.params, 'k*-', label='Right foot')\n",
    "    \n",
    "plt.legend()\n",
    "plt.xticks(ticks=(1,2,3))\n",
    "plt.xlabel('AR coefficient lag')\n",
    "plt.ylabel('AR coefficient value')\n",
    "plt.xlim(0,4)\n",
    "\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "67829e0d-dc0c-45bf-a8da-9cd90f757184",
   "metadata": {},
   "source": [
    "**Question 1.4.** On which coefficients is the separation between categories most promising?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceda346d-e521-4d7d-913f-d0ea18e95308",
   "metadata": {},
   "source": [
    "**Answer 1.4.** `Write your answer here`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57df1f6f-2692-41a0-a834-227b25487fa3",
   "metadata": {},
   "source": [
    "### Experiment 2: AR model evolution over time\n",
    "Real-life physiological signals can often vary substantially (and meaningfully) throughout a recording. It’s usually good practice to have a look at the data before trying to apply models. Consider the signal in `AF_sync.dat` – a recording of ECG atrial activity during atrial fibrillation (sampling frequency of 50 Hz). We start by importing the signal:"
   ]
  },
  {
   "cell_type": "code",
   "id": "66d8a5d9-303e-4c88-b9cd-be226b992d5e",
   "metadata": {},
   "source": [
    "with open(fafs, 'r') as f:\n",
    "    txt = f.readlines()\n",
    "    af_sync = np.array([float(s[:-1]) for s in txt])\n",
    "    \n",
    "af_sync -= np.mean(af_sync)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "2f48ca4c-caa8-433b-b8d4-20fc4787f58b",
   "metadata": {},
   "source": [
    "#### Changes across time\n",
    "Let's plot the signal and consider its evolution over the course of the recording. At the start (until sample ~2000 approximately), the signal is moderately organized; then it becomes very organized until sample ~3000. This probably corresponds to a drastic reduction in the number of fibrillatory waves in the atrial tissue (flutter). In the last part of the recording, the fibrillation, and thus the signal, becomes very disorganized."
   ]
  },
  {
   "cell_type": "code",
   "id": "b4a562bf-cec2-4c71-9d16-9a8c160faeed",
   "metadata": {},
   "source": [
    "fig = plt.figure(constrained_layout=True)\n",
    "ax = plt.axes()\n",
    "plt.plot(af_sync)\n",
    "plt.xlabel('Sample')\n",
    "plt.ylabel('Signal')\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "548697bf-28fd-4c37-b8a8-bbd44e252467",
   "metadata": {},
   "source": [
    "**Question 2.1.** The code below is intended to plot the signal again and mark the three periods described above, but it is not yet complete. Make the necessary modifications to show all three periods of interest."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b4b5189-b9b0-4a24-af5f-74211d54ffc2",
   "metadata": {},
   "source": [
    "**Answer 2.1.**"
   ]
  },
  {
   "cell_type": "code",
   "id": "45b969f6-9982-4597-b3fa-8dd9e6184ced",
   "metadata": {},
   "source": [
    "fig = plt.figure(constrained_layout=True)\n",
    "ax = plt.axes()\n",
    "plt.plot(af_sync)\n",
    "plt.xlabel('Sample')\n",
    "plt.ylabel('Signal')\n",
    "\n",
    "segments = [[1170, 2000], [2170, 2890], [3060, 4060]]\n",
    "eclr = [[1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 0.0]]\n",
    "\n",
    "ax.axvspan(segments[0][0], segments[0][1], ymin=0.01, ymax=0.99, ec=eclr[0], fill=False)\n",
    "\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "ac17e44d-feeb-4002-8009-f482003a1b57",
   "metadata": {},
   "source": [
    "#### Adaptive modeling\n",
    "The clearly different states in `af_sync`, which vary across time, can be studied and modeled more quantitatively using a sliding-window approach. We thereby consider a segmentation of the signal into 500-sample windows with 50% overlap. For each segment, we can then estimate (i) the signal variance, (ii) optimal AR order, and (iii) the AR coefficients & excitation variance, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "id": "46f1dece-c1a7-4391-94d1-9e9d5dde5493",
   "metadata": {},
   "source": [
    "nw = 500\n",
    "nv = round(nw*0.50)\n",
    "nt = len(af_sync)\n",
    "\n",
    "kt = []\n",
    "ki, kf = 0, nw\n",
    "\n",
    "# Getting the border and middle indices for each segment\n",
    "while True:\n",
    "    kt.append([ki, round(0.5*(ki+kf)), kf])\n",
    "    ki += nw - nv\n",
    "    kf += nw - nv\n",
    "    if kf > nt: break\n",
    "\n",
    "nc = len(kt)\n",
    "arv = np.zeros((nc,))\n",
    "aro = np.zeros((nc,), dtype=int)\n",
    "arr = np.zeros((nc,))\n",
    "nmax = 40\n",
    "\n",
    "# Modeling the signal for each segment\n",
    "for kc in range(nc):\n",
    "    \n",
    "    ys = af_sync[kt[kc][0]:kt[kc][2]]\n",
    "\n",
    "    arv[kc] = np.var(ys)\n",
    "    aro[kc], _, _ = ar_order(ys, nmax)\n",
    "    \n",
    "    _, sigma = yule_walker(ys, order=int(aro[kc]), method=\"mle\")\n",
    "    arr[kc] = sigma**2 / arv[kc]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a12b59fa-c586-40a6-b038-92894f387cbc",
   "metadata": {},
   "source": [
    "We can then plot together the time evolution of the raw signal, the signal variance, the AR order, and the ratio of excitation variance to signal variance."
   ]
  },
  {
   "cell_type": "code",
   "id": "5434894e-fa0f-462b-8016-f4a466a9308b",
   "metadata": {},
   "source": [
    "fig = plt.figure(constrained_layout=True)\n",
    "t = np.array(kt)[:,1]\n",
    "\n",
    "plt.subplot(4,1,1)\n",
    "plt.plot(af_sync)\n",
    "plt.ylabel('Original signal')\n",
    "plt.xlim(0, len(af_sync))\n",
    "\n",
    "plt.subplot(4,1,2)\n",
    "plt.plot(t, arv, 'o-')\n",
    "plt.ylabel('Signal var')\n",
    "plt.xlim(0, len(af_sync))\n",
    "\n",
    "plt.subplot(4,1,3)\n",
    "plt.plot(t, aro, 'o-')\n",
    "plt.ylabel('AR order')\n",
    "plt.xlim(0, len(af_sync))\n",
    "\n",
    "plt.subplot(4,1,4)\n",
    "plt.plot(t, arr, 'o-')\n",
    "plt.ylabel('Excitation var / signal var')\n",
    "plt.xlim(0, len(af_sync))\n",
    "plt.xlabel('Samples')\n",
    "\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "3b84bffd-58ed-4fee-b48c-ca0386ac9ab7",
   "metadata": {},
   "source": [
    "**Question 2.2.** Interpret the time evolution of the parameters plotted above, and how they relate to the organization of the signal in the three afore-mentioned stages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f45634-8908-475f-9eb3-64e067d33d95",
   "metadata": {},
   "source": [
    "**Answer 2.2.** `Write your answer here`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d3e6a29-a470-4bbd-8345-14add633d76a",
   "metadata": {},
   "source": [
    "#### Signal stability and organization\n",
    "Finally, another way of evaluating how organized a signal is, is by looking at its equivalent filter transfer function *H(z)* and the positioning of the corresponding poles. We can focus specifically on the three states defined previously in `segments`, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "id": "d1fc64e7-58fa-4df9-853e-bd42120a1844",
   "metadata": {},
   "source": [
    "roots = []\n",
    "\n",
    "for seg in segments:\n",
    "    \n",
    "    ys = af_sync[seg[0]:seg[1]]\n",
    "    \n",
    "    aro, _, _ = ar_order(ys, nmax)\n",
    "    rho, _ = yule_walker(ys, order=int(aro), method=\"mle\")\n",
    "    \n",
    "    rho = np.concatenate((-rho[::-1], np.array([1])), axis=0)\n",
    "    roots.append(1 / np.roots(rho))\n",
    "\n",
    "fig = plt.figure(constrained_layout=True)\n",
    "t = np.linspace(0, 2*np.pi, 1000)\n",
    "titles = ['Moderately organized', 'Fully organized', 'Disorganized']\n",
    "\n",
    "for k in range(3):\n",
    "    plt.subplot(1,3,k+1)\n",
    "    plt.plot(np.cos(t), np.sin(t))\n",
    "    plt.plot(np.real(roots[k]), np.imag(roots[k]), 'ro')\n",
    "    plt.axis('square')\n",
    "    plt.xlim([-1.5, 1.5])\n",
    "    plt.ylim([-1.5, 1.5])\n",
    "    plt.title(titles[k])\n",
    "\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "3cc15884-2bf4-41e5-9229-d55b417f5f9a",
   "metadata": {},
   "source": [
    "And the proximity to the unit circle can be further summarized in terms of the average magnitude:"
   ]
  },
  {
   "cell_type": "code",
   "id": "00840add-14ef-44cb-8847-77e303bd3474",
   "metadata": {},
   "source": [
    "for kr in range(len(roots)):\n",
    "    print(titles[kr] + ': {:.3f}'.format(np.mean(np.abs(roots[kr]))))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "729ce564-0486-44ed-9e7d-ae85e8cebf3a",
   "metadata": {},
   "source": [
    "**Question 2.3.** Comment on how signal organization relates to pole location (proximity to the circle)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c0805d-d13d-4e9f-9696-7c13ce4fc791",
   "metadata": {},
   "source": [
    "**Answer 2.3.** `Write your answer here`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab20bb58-a9f6-4c67-9bc6-77a68c232bb0",
   "metadata": {},
   "source": [
    "### Experiment 3: recovering the excitation (whitening filter)\n",
    "The signal in `speech.dat` corresponds to the spoken sound */a/*, sampled at 8 kHz. Used in language, we expect this signal to be clearly structured. We start by importing and plotting it:"
   ]
  },
  {
   "cell_type": "code",
   "id": "10051e14-5361-4c20-8348-1f1a04ff0097",
   "metadata": {},
   "source": [
    "with open(fspc, 'r') as f:\n",
    "    txt = f.readlines()\n",
    "    speech = np.array([float(s[:-1]) for s in txt])\n",
    "\n",
    "speech -= np.mean(speech)\n",
    "\n",
    "fig = plt.figure(constrained_layout=True)\n",
    "ax = plt.axes()\n",
    "plt.plot(speech)\n",
    "plt.xlabel('Sample')\n",
    "plt.ylabel('Signal')\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "2404af92-04d1-4b4e-8ee8-71373db422ca",
   "metadata": {},
   "source": [
    "Using the functions `ar_order` and `yule_walker` introduced before, we can estimate the optimal model order for this signal, and then the corresponsing model parameters:"
   ]
  },
  {
   "cell_type": "code",
   "id": "9d1cd556-f150-499e-bc3b-4c6b396cc20a",
   "metadata": {},
   "source": [
    "aro1, _, _ = ar_order(speech, 40)\n",
    "rho1, sgm1 = yule_walker(speech, order=int(aro1), method=\"mle\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "89f2358a-ab6e-4a4c-af80-0e3928e47fad",
   "metadata": {},
   "source": [
    "#### The underlying excitation signal\n",
    "In the framework of AR modeling, the excitation signal driving the observed speech signal can be estimated using a filtering step as follows:"
   ]
  },
  {
   "cell_type": "code",
   "id": "de81622a-d5b3-446b-8983-324463f69f3b",
   "metadata": {},
   "source": [
    "exc1 = lfilter(np.concatenate(([1], -rho1), axis=0), [1], speech)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "3262464e-684f-4180-b8f8-9e96bb42100b",
   "metadata": {},
   "source": [
    "**Question 3.1.** Explain why the excitation can be estimated in this way."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb807e58-9580-4d4c-8557-596bd6b591c6",
   "metadata": {},
   "source": [
    "**Answer 3.1.** `Write your answer here`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb89a3d-aadb-4c32-b525-8339023a4adc",
   "metadata": {},
   "source": [
    "We can now visualize the estimated excitation signal:"
   ]
  },
  {
   "cell_type": "code",
   "id": "a20554ea-9772-4354-977d-a9b669397795",
   "metadata": {},
   "source": [
    "fig = plt.figure(constrained_layout=True)\n",
    "plt.subplot(2,1,1)\n",
    "plt.plot(speech)\n",
    "plt.title('Original signal')\n",
    "plt.xlim(0, len(speech))\n",
    "\n",
    "plt.subplot(2,1,2)\n",
    "plt.plot(exc1)\n",
    "plt.title('Estimated excitation')\n",
    "plt.xlim(0, len(speech))\n",
    "plt.xlabel('Samples')\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a5aed6e8-b142-4b6c-8d2d-f19801d0b768",
   "metadata": {},
   "source": [
    "**Question 3.2.** Compare the excitation with the speech signal. Does the excitation look like white noise?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3838a586-0727-4ae3-a630-2cd78dad546a",
   "metadata": {},
   "source": [
    "**Answer 3.2.** `Write your answer here`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a961d2c0-68f1-4f4f-b5be-a01f75838480",
   "metadata": {},
   "source": [
    "We can test more objectively whether the excitation is indeed similar to white noise by looking at its normalized autocorrelation, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "id": "197f03b2-4524-45c4-aefd-7905c02bb0fd",
   "metadata": {},
   "source": [
    "def test_white(x, Aff=0):\n",
    "    \"\"\"\n",
    "    Computation of the ratio of normalized autocorrelation estimates \n",
    "    larger than a 5% threshold\n",
    "    x: signal\n",
    "    Aff: 0 no graphic display; 1 display\n",
    "    \"\"\"\n",
    "    \n",
    "    K = len(x)\n",
    "    \n",
    "    # Calculate the biased autocorrelation of the signal\n",
    "    v = np.correlate(x, x, mode='full') / K\n",
    "    # Note: K-1 is the index for zero lag\n",
    "    \n",
    "    thresh = 1.96 / np.sqrt(K)\n",
    "    pc = np.sum(np.abs(v[K:] / v[K-1]) > thresh) / (K-1)\n",
    "    \n",
    "    if Aff == 1:\n",
    "        fig = plt.figure(constrained_layout=True)\n",
    "        ml = K-1 #min(30, K-1)\n",
    "        lags = range(-ml, ml + 1)\n",
    "        corr = v[K-1 - ml : K-1 + ml + 1] / v[K-1]\n",
    "        plt.stem(lags, corr)\n",
    "        plt.plot([-ml, ml], [+thresh, +thresh], 'r')\n",
    "        plt.plot([-ml, ml], [-thresh, -thresh], 'r')\n",
    "        plt.title('Estimated normalized correlation and 95% confidence interval')\n",
    "        plt.show()\n",
    "\n",
    "    return pc\n",
    "\n",
    "print(\"Proportion above 5% threshold: {:f}\".format(test_white(exc1, Aff=1)))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "801e5823-4176-4eff-91b2-96a4e7770897",
   "metadata": {},
   "source": [
    "**Question 3.3.** What can we say based on this result?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a57720-070b-4ecb-80a1-38893ec00c40",
   "metadata": {},
   "source": [
    "**Answer 3.3.** `Write your answer here`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3980d309-5abf-494d-90d5-46ed34f59ea1",
   "metadata": {},
   "source": [
    "We now consider a different example: a timeseries of daily blood systolic pressure recorded from a patient, stored in `blood.dat`:"
   ]
  },
  {
   "cell_type": "code",
   "id": "7a4eb1a6-7034-43d8-b91d-fd0909b324f9",
   "metadata": {},
   "source": [
    "with open(fbld, 'r') as f:\n",
    "    txt = f.readlines()\n",
    "    txt = [s[:-1].split() for s in txt]\n",
    "    blood = np.array([float(s[0]) for s in txt])\n",
    "\n",
    "blood -= np.mean(blood)\n",
    "\n",
    "fig = plt.figure(constrained_layout=True)\n",
    "ax = plt.axes()\n",
    "plt.plot(blood)\n",
    "plt.xlabel('Sample')\n",
    "plt.ylabel('Signal')\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "53d5ee8f-0510-4078-97e6-65bc39324ba9",
   "metadata": {},
   "source": [
    "**Question 3.4.** Repeat the full analysis done for the speech signal (i.e. estimating AR order, model parameters, excitation signal, whiteness test). How does this excitation signal compare to that of the speech example?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca18cba-3a74-4b21-9e74-5459bf015a9b",
   "metadata": {},
   "source": [
    "**Answer 3.4.** `Write your answer here`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47901216-72ae-4ad8-afa7-a87d7bc6cd74",
   "metadata": {},
   "source": [
    "### Have a good session, and don't hesitate to ask questions!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
